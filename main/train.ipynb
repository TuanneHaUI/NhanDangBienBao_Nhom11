{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BƯỚC 1: Tạo class mapping ===\n",
      "Số lượng classes: 76\n",
      "Classes: ['complementary--chevron-left', 'complementary--chevron-right', 'complementary--distance', 'information--disabled-persons', 'information--gas-station', 'information--hospital', 'information--motorway', 'information--parking', 'information--tram-bus-stop', 'regulatory--axel-mass-limit', 'regulatory--bicycles-only', 'regulatory--go-straight', 'regulatory--go-straight-or-turn-left', 'regulatory--go-straight-or-turn-right', 'regulatory--height-limit', 'regulatory--keep-left', 'regulatory--keep-right', 'regulatory--maximum-speed-limit', 'regulatory--no-bicycles', 'regulatory--no-buses', 'regulatory--no-entry', 'regulatory--no-goods-vehicles', 'regulatory--no-goods-vehicles-exceeding-limit', 'regulatory--no-heavy-goods-vehicles', 'regulatory--no-left-turn', 'regulatory--no-motor-vehicles', 'regulatory--no-overtaking', 'regulatory--no-parking', 'regulatory--no-pedestrians', 'regulatory--no-right-turn', 'regulatory--no-u-turn', 'regulatory--pass-on-either-side', 'regulatory--pedestrians-only', 'regulatory--roundabout', 'regulatory--shared-path-pedestrians-and-bicycles', 'regulatory--stop', 'regulatory--turn-left', 'regulatory--turn-left-or-right', 'regulatory--turn-right', 'regulatory--weight-limit', 'regulatory--yield', 'regulatory--yield-to-oncoming-traffic', 'warning--bicycles-crossing', 'warning--children', 'warning--crossroads', 'warning--curve-left', 'warning--curve-right', 'warning--delineator', 'warning--delineator--left', 'warning--delineator--right', 'warning--domestic-animals', 'warning--double-curve-first-left', 'warning--double-curve-first-right', 'warning--falling-rocks-or-debris-left', 'warning--falling-rocks-or-debris-right', 'warning--junction-with-a-side-road-perpendicular-left', 'warning--junction-with-a-side-road-perpendicular-right', 'warning--other-danger', 'warning--pedestrians-crossing', 'warning--railroad-crossing-with-barriers', 'warning--railroad-crossing-without-barriers', 'warning--road-bump', 'warning--road-dip', 'warning--road-narrows', 'warning--road-narrows-left', 'warning--road-narrows-right', 'warning--road-slope-left', 'warning--road-slope-right', 'warning--roadworks', 'warning--roundabout', 'warning--slippery-road-surface', 'warning--traffic-merges-left', 'warning--traffic-merges-right', 'warning--traffic-signals', 'warning--uneven-road', 'warning--wild-animals']\n",
      "\n",
      "=== BƯỚC 2: Chia dữ liệu ===\n",
      "Train: 22355 files\n",
      "Validation: 3193 files\n",
      "Test: 6388 files\n",
      "\n",
      "=== BƯỚC 3: Tạo datasets và dataloaders ===\n",
      "\n",
      "=== BƯỚC 4: Tạo mô hình ===\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BƯỚC 5: Bắt đầu training ===\n",
      "Loading checkpoint D:\\xla v1\\model_output\\checkpoint\\checkpoint (1).pth\n",
      "Loaded checkpoint from epoch 19 with val_loss 0.02996216141588121\n",
      "Total training time: 0.00 seconds\n",
      "\n",
      "=== BƯỚC 6: Lưu kết quả ===\n",
      "Lưu mô hình thành công\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "processed_dir = r\"D:\\xla v1\\code\\tienxuli\\archive\"\n",
    "images_dir = os.path.join(processed_dir, \"images\")\n",
    "annotations_dir = os.path.join(processed_dir, \"xmls\")\n",
    "\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, image_files, class_to_idx, transforms=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_files = image_files\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.images_dir, img_name)\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Lỗi: File ảnh không tồn tại: {img_path}\")\n",
    "                return None\n",
    "\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            xml_name = img_name.replace('.jpg', '.xml')\n",
    "            xml_path = os.path.join(self.annotations_dir, xml_name)\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f\"Lỗi: File XML không tồn tại: {xml_path}\")\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = []\n",
    "                labels = []\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                for obj in root.findall('object'):\n",
    "                    name = obj.find('name').text\n",
    "                    bbox = obj.find('bndbox')\n",
    "\n",
    "                    xmin = float(bbox.find('xmin').text)\n",
    "                    ymin = float(bbox.find('ymin').text)\n",
    "                    xmax = float(bbox.find('xmax').text)\n",
    "                    ymax = float(bbox.find('ymax').text)\n",
    "\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        boxes.append([xmin, ymin, xmax, ymax])\n",
    "                        labels.append(self.class_to_idx[name])\n",
    "\n",
    "                if len(boxes) == 0:\n",
    "                    boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                else:\n",
    "                    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi trong __getitem__ với index {idx}: {e}\")\n",
    "            print(f\"Image path: {img_path}\")\n",
    "            print(f\"XML path: {xml_path}\")\n",
    "            return None\n",
    "\n",
    "def get_class_mapping():\n",
    "    class_names = set()\n",
    "\n",
    "    for xml_file in os.listdir(annotations_dir):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            xml_path = os.path.join(annotations_dir, xml_file)\n",
    "            try:\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                for obj in root.findall('object'):\n",
    "                    class_names.add(obj.find('name').text)\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi parse XML file {xml_path}: {e}\")\n",
    "\n",
    "    class_names = sorted(list(class_names))\n",
    "    class_to_idx = {name: idx + 1 for idx, name in enumerate(class_names)}\n",
    "    idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "    print(f\"Số lượng classes: {len(class_names)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "\n",
    "    return class_to_idx, idx_to_class, class_names\n",
    "\n",
    "def split_data(test_size=0.2, val_size=0.1):\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    train_files, temp_files = train_test_split(\n",
    "        image_files, test_size=test_size + val_size, random_state=42\n",
    "    )\n",
    "\n",
    "    val_files, test_files = train_test_split(\n",
    "        temp_files, test_size=test_size/(test_size + val_size), random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {len(train_files)} files\")\n",
    "    print(f\"Validation: {len(val_files)} files\")\n",
    "    print(f\"Test: {len(test_files)} files\")\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes + 1\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [data for data in batch if data is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    data_loader_tqdm = tqdm(data_loader, desc=f\"Epoch {epoch} (Training)\", leave=False)\n",
    "\n",
    "    for i, (images, targets) in enumerate(data_loader_tqdm):\n",
    "        if images is None or targets is None:\n",
    "            print(\"Bỏ qua batch vì có lỗi trong collate_fn\")\n",
    "            continue\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        data_loader_tqdm.set_postfix({\"loss\": losses.item()})\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, class_names, idx_to_class):\n",
    "    model.eval()\n",
    "    iou_thresholds = [0.5]\n",
    "    class_metrics = {class_name: {f'AP@{iou}': [] for iou in iou_thresholds}\n",
    "                    for class_name in class_names}\n",
    "    all_aps = []\n",
    "\n",
    "    data_loader_tqdm = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    for images, targets in data_loader_tqdm:\n",
    "        if images is None or targets is None:\n",
    "            print(\"Bỏ qua batch vì có lỗi trong collate_fn\")\n",
    "            continue\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, image_output in enumerate(outputs):\n",
    "            boxes = image_output['boxes'].cpu()\n",
    "            scores = image_output['scores'].cpu()\n",
    "            labels = image_output['labels'].cpu()\n",
    "            true_labels = targets[i]['labels'].cpu()\n",
    "            true_boxes = targets[i]['boxes'].cpu()\n",
    "\n",
    "            for iou_thresh in iou_thresholds:\n",
    "                tp = torch.zeros(len(boxes), dtype=torch.bool)\n",
    "                fp = torch.zeros(len(boxes), dtype=torch.bool)\n",
    "\n",
    "                if len(true_boxes) > 0:\n",
    "                    ious = box_iou(boxes, true_boxes)\n",
    "                    max_iou, argmax_iou = torch.max(ious, dim=1)\n",
    "                    tp[(max_iou >= iou_thresh)] = True\n",
    "                else:\n",
    "                    fp[:] = True\n",
    "\n",
    "                n_gt = len(true_labels)\n",
    "                n_detections = len(labels)\n",
    "\n",
    "                for class_idx in range(1, len(class_names) + 1):\n",
    "                    class_name = idx_to_class[class_idx]\n",
    "                    detections_of_class = labels == class_idx\n",
    "                    gt_of_class = true_labels == class_idx\n",
    "\n",
    "                    TP = torch.sum(tp[detections_of_class])\n",
    "                    FP = torch.sum(fp[detections_of_class])\n",
    "                    FN = torch.sum(gt_of_class) - TP\n",
    "\n",
    "                    precision = TP / (TP + FP) if TP + FP > 0 else torch.tensor(0.0)\n",
    "                    recall = TP / (TP + FN) if TP + FN > 0 else torch.tensor(0.0)\n",
    "\n",
    "                    ap = (precision + recall) / 2\n",
    "                    class_metrics[class_name][f'AP@{iou_thresh}'].append(ap.item())\n",
    "                    all_aps.append(ap.item())\n",
    "\n",
    "    mean_ap = sum(all_aps) / len(all_aps) if all_aps else 0.0\n",
    "\n",
    "    return mean_ap\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, val_loss, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=r\"D:\\xla v1\\model_output\\checkpoint\\checkpoint (1).pth\"):\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint {filename}\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['val_loss']\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch-1} with val_loss {best_val_loss}\")\n",
    "        return start_epoch, best_val_loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "def main():\n",
    "    global class_names, idx_to_class\n",
    "\n",
    "    print(\"=== BƯỚC 1: Tạo class mapping ===\")\n",
    "    class_to_idx, idx_to_class, class_names = get_class_mapping()\n",
    "\n",
    "    print(\"\\n=== BƯỚC 2: Chia dữ liệu ===\")\n",
    "    train_files, val_files, test_files = split_data()\n",
    "\n",
    "    print(\"\\n=== BƯỚC 3: Tạo datasets và dataloaders ===\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ObjectDetectionDataset(\n",
    "        images_dir, annotations_dir, train_files, class_to_idx, train_transforms\n",
    "    )\n",
    "    val_dataset = ObjectDetectionDataset(\n",
    "        images_dir, annotations_dir, val_files, class_to_idx, val_transforms\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== BƯỚC 4: Tạo mô hình ===\")\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = get_model(len(class_names))\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    print(\"\\n=== BƯỚC 5: Bắt đầu training ===\")\n",
    "    num_epochs = 18\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    start_epoch, best_val_loss = load_checkpoint(model, optimizer)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "            train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch+1)\n",
    "            train_losses.append(train_loss)\n",
    "            epoch_end_time = time.time()\n",
    "            print(f\"Epoch training time: {(epoch_end_time - epoch_start_time):.2f} seconds\")\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "            mean_ap = evaluate(model, val_loader, device, class_names, idx_to_class)\n",
    "            val_loss = mean_ap\n",
    "            val_losses.append(val_loss)\n",
    "            epoch_end_time = time.time()\n",
    "\n",
    "            print(f\"Val Loss (Mean AP): {val_loss:.4f}\")\n",
    "            print(f\"Epoch validation time: {(epoch_end_time - epoch_start_time):.2f} seconds\")\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if val_loss > best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                }, 'best_model.pth')\n",
    "                print(\"Saved best model!\")\n",
    "\n",
    "            save_checkpoint(epoch, model, optimizer, val_loss)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted. Saving checkpoint...\")\n",
    "        save_checkpoint(epoch, model, optimizer, val_loss, filename=\"interrupted_checkpoint.pth\")\n",
    "        print(\"Checkpoint saved. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    training_end_time = time.time()\n",
    "    total_training_time = training_end_time - training_start_time\n",
    "    print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\n=== BƯỚC 6: Lưu kết quả ===\")\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    print(\"Lưu mô hình thành công\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
